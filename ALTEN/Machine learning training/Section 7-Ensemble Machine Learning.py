# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/184Z2jPaLKLOG4RB6dR0nOB9Or7gywvpW
"""

### TO BE RUN ON GOOGLE COLAB, Use the .ipynb

##Emsamble methods

# Bagging
#Bootstrap sampling: Sampling with replacement
#Combine by averiging the output (regression)
#Combine by voting (classification)
#Can be applied to many classifiers which inlcudes ANN, CART, etc

# Pasting
#Sampling without replacement

# Boosting
#Train week classifiers
#Add them to a final strong classfier by weighting. Weighting by accuracy (typically)
#Once added, the data are reweighted
  # misclassified samples gain weight
  # correctly classified samples lose weight (exception: Boost by majority and BrownBoost - decrease the wegiht of repeatedly misclassfied examples)
  # Algo are forced to learn more from misclassified samples

# Stacking
#Stacked generalization
#Combined information from multiple predictive models to generate a new model
#Step 1: Train learning alo
#Step 2: Combiner algo is trained using algo predictions from step 1

# Bayes optimal classifier
# Bayesian parameter averaging
# Bayesian model combination
# Bucket of models

#%% Bagging

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

df = sns.load_dataset('titanic')
df.head()

#%%

df.shape

#df.dropna(inplace=True)
df['pclass'].unique()

df['pclass'].value_counts()

df['sex'].value_counts()

df.shape

#%% Data preprocessing
subset = df[['pclass','sex','age','survived']].copy()
subset.dropna(inplace=True)
X = subset[['pclass','sex','age']].copy()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

X['sex'] = le.fit_transform(subset['sex'])

X.head()

X.describe()

Y = subset['survived'].copy()
Y.value_counts()

#%% Fit Model
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)

from sklearn import preprocessing
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix, roc_auc_score

def print_score(clf, X_train, X_test, Y_train, Y_test, train=True):
    lb = preprocessing.LabelBinarizer()
    lb.fit(Y_train)
    if train:

      res = clf.predict(X_train)
      print("Train Result:\n================================================")
      print("accuracy score: {0:.4f}\n".format(accuracy_score(Y_train, res)))
      print("Classification Report: {}\n".format(classification_report(Y_train, res)))
      print("Confusion matrix: {}\n".format(confusion_matrix(Y_train, res)))
      print("ROC AUC: {0:.4f}\n".format(roc_auc_score(lb.transform(Y_train), lb.transform(res))))

      res = cross_val_score(clf, X_train, Y_train, cv=10, scoring='accuracy')
      print("Average Accuracy: \t {0:.4f}".format(np.mean(res)))
      print("Accuracy SD: \t\t {0:.4f}".format(np.std(res)))
      print("------------------------------------------------------------------------------------------------------")

    elif train==False:
      res_test = clf.predict(X_test)
      print("Test Result:\n================================================")
      print("accuracy score: {0:.4f}\n".format(accuracy_score(Y_test, res_test)))
      print("Classification Report: {}\n".format(classification_report(Y_test, res_test)))
      print("Confusion matrix: {}\n".format(confusion_matrix(Y_test, res_test)))
      print("ROC AUC: {0:.4f}\n".format(roc_auc_score(lb.transform(Y_test), lb.transform(res_test))))

      #res_test = cross_val_score(clf, X_test, Y_test, cv=10, scoring='accuracy')
      #print("Average Accuracy: \t {0:.4f}".format(np.mean(res_test)))
      #print("Accuracy SD: \t\t {0:.4f}".format(np.std(res_test)))
      print("------------------------------------------------------------------------------------------------------")

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, Y_train)
print_score(clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(clf,X_train,X_test,Y_train,Y_test,train=False)

#%% Bagging (oob_Score=False)

bag_clf = BaggingClassifier(estimator=clf, n_estimators=1000, bootstrap=True, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, Y_train)

print_score(bag_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(bag_clf,X_train,X_test,Y_train,Y_test,train=False)

#%% Bagging (oob_Score=True)

bag_clf = BaggingClassifier(estimator=clf, n_estimators=1000, bootstrap=True, oob_score=True, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, Y_train)

print_score(bag_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(bag_clf,X_train,X_test,Y_train,Y_test,train=False)

bag_clf.oob_score_

#%% Bagging with Random Forest
# Random Forrest
#Ensemble of Decission Trees
#Training via the bagging method (Repeated sampling with replacement)
  #Bagging: Sample from samples
  #RF: Sample from predictions. m = sqrt(p) for classification and m = p/3 for regression problems
#Utilise uncorrelated trees
#RF: Sample both observatins and features of training data
#Bagging: Samples only observations at random, Decission tree select best feature when splitting a node

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)

rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)
rf_clf.fit(X_train, Y_train)

print_score(rf_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(rf_clf,X_train,X_test,Y_train,Y_test,train=False)

# Grid Search to seek out the best hyperparameters for our ML model
from sklearn.pipeline import Pipeline

from sklearn.model_selection import GridSearchCV

rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)

params_grid = {"max_depth": [3, None],
              "min_samples_split": [2, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [True, False],
              "criterion": ["gini", "entropy"]}

grid_search = GridSearchCV(rf_clf, params_grid, n_jobs = -1, cv = 5, verbose = 1,scoring='accuracy')
grid_search.fit(X_train, Y_train)

grid_search.best_score_

grid_search.best_estimator_.get_params()

print_score(grid_search,X_train,X_test,Y_train,Y_test,train=True)
print_score(grid_search,X_train,X_test,Y_train,Y_test,train=False)

# Extra-Trees (Extremely Randomized Trees Ensemble)
#Random Forrest is build uppon Decission Tree
#Decission Tree node splitting is based on gini or entropy or some other algorithms
#Extra trees make use of random thresholds for each feature unlike Decission Tree

from sklearn.ensemble import ExtraTreesClassifier

xt_clf = ExtraTreesClassifier(random_state=42, n_estimators=100)
xt_clf.fit(X_train, Y_train)

print_score(xt_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(xt_clf,X_train,X_test,Y_train,Y_test,train=False)

# Boosting (AdaBoost)

#The algo could learn from past mistakes by focussing more on difficult problems.

from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(random_state=42, n_estimators=100)
ada_clf.fit(X_train, Y_train)

print_score(ada_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(ada_clf,X_train,X_test,Y_train,Y_test,train=False)

#AdaBoost with RF
from sklearn.ensemble import RandomForestClassifier
ada_clf = AdaBoostClassifier(RandomForestClassifier(random_state=42, n_estimators=100), random_state=42, n_estimators=100)
ada_clf.fit(X_train, Y_train)

print_score(ada_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(ada_clf,X_train,X_test,Y_train,Y_test,train=False)

# Gradient boosting
# Sequentl adding predictors

from sklearn.ensemble import GradientBoostingClassifier

gbc_clf = GradientBoostingClassifier(random_state=42, n_estimators=100)
gbc_clf.fit(X_train, Y_train)

print_score(gbc_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(gbc_clf,X_train,X_test,Y_train,Y_test,train=False)

# Extreme Gradient Boosting XGBoost
#XGBoost used a more regularized model formalization to control over-fitting, which gives it better performance
#For model, it might be more suitable to be called as regularized gradient boosting
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)
import xgboost as xgb

xgb_clf = xgb.XGBClassifier(max_depth=5, n_estimators=10000, learning_rate=0.3,n_jobs=-1)
xgb_clf.fit(X_train, Y_train)

print_score(xgb_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(xgb_clf,X_train,X_test,Y_train,Y_test,train=False)

# Project HR

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
df.head()

num_col = list(df.describe().columns) #numerical columns
col_categorical = list(set(df.columns).difference(num_col)) #categorical columns
remove_list = ['EmployeeCount','EmployeeNumber','StandardHours'] #Useless columns
col_numerical = [e for e in num_col if e not in remove_list]
df['Attrition_num'] = df['Attrition'].map({'Yes':0,'No':1})
col_categorical.remove('Attrition')
df_cat = pd.get_dummies(df[col_categorical])
X = pd.concat([df[col_numerical],df_cat],axis=1)
Y = df['Attrition_num']

#Decission Tree
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)

clf.fit(X_train, Y_train)

X_train.shape

print_score(clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(clf,X_train,X_test,Y_train,Y_test,train=False)

#Bagging
from sklearn.ensemble import BaggingClassifier

bag_clf = BaggingClassifier(estimator=clf, n_estimators=100, bootstrap=True, oob_score=False, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, Y_train)

print_score(bag_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score(bag_clf,X_train,X_test,Y_train,Y_test,train=False)

#Random Forrest
from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=100)
rf_clf.fit(X_train, Y_train.ravel())

print_score (rf_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score (rf_clf,X_train,X_test,Y_train,Y_test,train=False)

import seaborn as sns
pd.Series(rf_clf.feature_importances_, index=X.columns).sort_values(ascending=False).plot.bar()

#AdaBoost Tree
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier()
ada_clf.fit(X_train, Y_train)

print_score (ada_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score (ada_clf,X_train,X_test,Y_train,Y_test,train=False)

#AdaBoost + RF
ada_clf = AdaBoostClassifier(RandomForestClassifier(n_estimators=100),n_estimators=100)
ada_clf.fit(X_train, Y_train)

print_score (ada_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score (ada_clf,X_train,X_test,Y_train,Y_test,train=False)

#Gradient Boosting Classifier
from sklearn.ensemble import GradientBoostingClassifier

gbc_clf = GradientBoostingClassifier(n_estimators=100)
gbc_clf.fit(X_train, Y_train)

print_score (gbc_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score (gbc_clf,X_train,X_test,Y_train,Y_test,train=False)

#XGboost
import xgboost as xgb

xgb_clf = xgb.XGBClassifier(n_estimators=100)
xgb_clf.fit(X_train, Y_train)

print_score (xgb_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score (xgb_clf,X_train,X_test,Y_train,Y_test,train=False)

# Ensemble of Ensembles - Model stacking
#Use the probability prediction of several other ensemble methods

en_en = pd.DataFrame()

en_en['tree_clf'] = pd.DataFrame(clf.predict_proba(X_train))[1]
en_en['rf_clf'] = pd.DataFrame(rf_clf.predict_proba(X_train))[1]
col_name = en_en.columns
en_en = pd.concat([en_en, pd.DataFrame(Y_train).reset_index(drop=True)],axis=1)
en_en.head()

tmp = list(col_name)
tmp.append('ind')
en_en.columns = tmp
en_en

#Meta Classifier
from sklearn.linear_model import LogisticRegression

m_clf = LogisticRegression(fit_intercept=False, solver='lbfgs')
m_clf.fit(en_en[['tree_clf','rf_clf']], en_en['ind'])

en_test = pd.DataFrame()

en_test['tree_clf'] = pd.DataFrame(clf.predict_proba(X_test))[1]
en_test['rf_clf'] = pd.DataFrame(rf_clf.predict_proba(X_test))[1]
col_name = en_en.columns
en_test['combined'] = m_clf.predict(en_test[['tree_clf','rf_clf']])

col_name = en_test.columns
tmp = list(col_name)
tmp.append('ind')

en_test = pd.concat([en_test, pd.DataFrame(Y_test).reset_index(drop=True)],axis=1)
en_test.columns = tmp
en_test

print(pd.crosstab(en_test['ind'],en_test['combined']))

from sklearn.metrics import accuracy_score, classification_report
print(round(accuracy_score(en_test['ind'],en_test['combined']),4))
print(classification_report(en_test['ind'],en_test['combined']))

df.Attrition.value_counts()/df.Attrition.count()

from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier
class_weight = {0:0.839,1:0.161}

pd.Series(list(Y_train)).value_counts() / pd.Series(list(Y_train)).count()

forest = RandomForestClassifier(n_estimators=100, class_weight=class_weight)
ada = AdaBoostClassifier(estimator=forest, n_estimators=100,learning_rate=0.5,random_state=42)
ada.fit(X_train, Y_train)

print_score (ada,X_train,X_test,Y_train,Y_test,train=True)
print_score (ada,X_train,X_test,Y_train,Y_test,train=False)

bag_clf = BaggingClassifier(estimator=ada, n_estimators=100, bootstrap=True, oob_score=False, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, Y_train)

print_score (bag_clf,X_train,X_test,Y_train,Y_test,train=True)
print_score (bag_clf,X_train,X_test,Y_train,Y_test,train=False)

