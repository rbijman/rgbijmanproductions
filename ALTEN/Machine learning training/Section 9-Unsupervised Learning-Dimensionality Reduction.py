# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sMgtZV0WktHzl_uXDEwY_KJegdDwJ5Rt
"""

# Unsupervised Learning:
#Dimensionality Reduction
  # Principal Component Analysis (PCA)
    # Linear dimensionality reduction using Singula Value Decomposition of the data to project it to a lower dimensional space.
    # Statistical procedure that utilise orthogonal transformation technology
    # Convert possible correlated features (predictors) into linearly uncorrelated features (predictors) called principal components
    # # of principal components <= number of features (predictors)
    # first principal component explains the largest possible variance
    # each subsequent component has the heighest variance subject to the resriction that it must be orthogonal to the precending components.
    # a collection of the components are called vectors
    # senstitive to scaling
  # Linear Discriminant Analysis (LDA)
    # Most commonly used as dimenstionality reduction technique in the pre-processing step for pattern-classification and machine learning applications
    # Goal is to project a dataset onto a lower-dimensional space with good class-seperability in order to avoid overfitting ("curse of dimensionality") and also reduce computational costs
    # Locate the 'boundaries' around clusters of classes
    # Project data points on a line
    # A centroid will be allocated to each cluster or have a centroid nearby

# PCA (Linear)
  # Used in exploratory data analysis (EDA)
  # Visualize genetic distance and relatedness between populations
  # Method:
    # Eigenvalue decomposition of a data covarience (or correlation) matrix
    # Singular value decomposition of a data matrix (After mean centering/normalizing) the data matrix for each attribute
  # Output
    # Component scores, sometimes called factor scores (the transformed variable values)
    # loadings (the weight)
  # Data compression and information preservation
  # Visualization
  # Noise filtering
  # Feature extraction and engineering

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

rnd_num = np.random.RandomState(42)
X = np.dot(rnd_num.rand(2,2),rnd_num.randn(2,500)).T

X.shape

plt.scatter(-X[:,0],X[:,1])
plt.axis('equal')

#Principal Components
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X)

print(pca.components_)

print(pca.explained_variance_)

print(pca.explained_variance_ratio_)

pca = PCA(n_components=1)
pca.fit(X)
X_pca = pca.transform(X)
X_pca.shape

X_new = pca.inverse_transform(X_pca)

plt.scatter(X[:,0],X[:,1])
plt.scatter(X_new[:,0],X_new[:,1],alpha=0.8)
plt.axis('equal')

# Project Wine
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data

df = pd.read_csv('wine.data',header=None)
df.head()

col_name = ['Class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']
df.columns = col_name

df.head()

X = df.iloc[:,1:]
X.head()

Y = df['Class']
Y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_sc = sc.fit_transform(X_train)
X_test_sc = sc.transform(X_test)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X_train_sc)

pca.explained_variance_ratio_

print(np.round(pca.explained_variance_ratio_,3))

pd.DataFrame(np.round(pca.components_,3),columns=X.columns).T

pca = PCA(n_components=None) #if you don't know how many components to use
pca.fit(X_train_sc)

pca.transform(X_train_sc)

np.cumsum(pca.explained_variance_ratio_)

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')

res = pca.transform(X_train_sc)
index_name = ['PCA_'+str(k) for k in range(0,len(res))]

df1 = pd.DataFrame(res,columns=df.columns[1:],index=index_name)[0:4]

df1.T.sort_values(by='PCA_0')

# Kernel PCA
#non-linear dimensionality reduction through the use of kernals

from sklearn.datasets import make_circles
from sklearn.decomposition import PCA, KernelPCA

np.random.seed(0)
X,Y = make_circles(n_samples=400, factor=.3, noise=.05)

kpca = KernelPCA(kernel='rbf', fit_inverse_transform=True, gamma=10)
X_kpca = kpca.fit_transform(X)
X_back = kpca.inverse_transform(X_kpca)

pca = PCA()
X_pca = pca.fit_transform(X)

#Normal plot
plt.figure()
plt.title('original space')
reds = Y==0
blues = Y==1

plt.scatter(X[reds,0],X[reds,1],c="red",s=20,edgecolors='k')
plt.scatter(X[blues,0],X[blues,1],c="blue",s=20,edgecolors='k')
plt.xlabel('$x_1$')
plt.ylabel('$x_2$')

#PCA plot
plt.scatter(X_pca[reds,0],X_pca[reds,1],c="red",s=20,edgecolors='k')
plt.scatter(X_pca[blues,0],X_pca[blues,1],c="blue",s=20,edgecolors='k')
plt.title("Projection by PCA")
plt.xlabel('1st principal component')
plt.ylabel('2nd component')

#KPCA plot
plt.scatter(X_kpca[reds,0],X_kpca[reds,1],c="red",s=20,edgecolors='k')
plt.scatter(X_kpca[blues,0],X_kpca[blues,1],c="blue",s=20,edgecolors='k')
plt.title("Projection by PCA")
plt.xlabel('1st principal component')
plt.ylabel('2nd component')

# Kernal PCA Example

df = sns.load_dataset('iris')
df = df[df['species'] != 'setosa']

col = ['petal_length','petal_width']
X = df.loc[:,col]
species_to_num = {'virginica':1,'versicolor':0}
df['tmp'] = df['species'].map(species_to_num)
Y = df['tmp']

kpca = KernelPCA(kernel='rbf', n_components=2)
X_kpca = kpca.fit_transform(X)

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(solver='lbfgs')
clf.fit(X,Y)

Xv = X.values.reshape(-1,1)
h = 0.02
x_min, x_max = Xv.min(), Xv.max() + 1
y_min, y_max = Y.min(), Y.max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
ax = plt.contour(xx,yy,Z,cmap='afmhot',alpha=0.3)
plt.scatter(X.values[:,0],X.values[:,1],c=Y,s=80,edgecolors='g',alpha=0.9)
plt.xlabel('petal length')
plt.ylabel('petal width')

clf.fit(X_kpca,Y)

Xv = X_kpca
h = 0.02
x_min, x_max = Xv.min()-0.5, Xv.max() + 0.5
y_min, y_max = Y.min()-0.5, Y.max()
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
ax = plt.contour(xx,yy,Z,cmap='afmhot',alpha=0.3)
plt.scatter(X_kpca[:,0],X_kpca[:,1],c=Y,s=80,edgecolors='g',alpha=0.9)
plt.xlabel('petal length')
plt.ylabel('petal width')

# LDA vs PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

clf = LinearDiscriminantAnalysis()
clf.fit(X,Y)

Xv = X.values.reshape(-1,1)
h = 0.02
x_min, x_max = Xv.min(), Xv.max() + 1
y_min, y_max = Y.min(), Y.max() + 2

xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
ax = plt.contour(xx,yy,Z,cmap='afmhot',alpha=0.3)
plt.scatter(X.values[:,0],X.values[:,1],c=Y,s=80,edgecolors='g',alpha=0.9)
plt.xlabel('petal length')
plt.ylabel('petal width')

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
#Project Abalone
!wget http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data

df1 = pd.read_csv('abalone.data',header=None)

df1.head()

X = df1.iloc[:,1:]

X.head()

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
from sklearn.decomposition import PCA
pca = PCA(n_components=None)
X_sc = sc.fit_transform(X)
pca.fit(X_sc)
np.cumsum(pca.explained_variance_ratio_)

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')

