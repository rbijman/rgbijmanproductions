# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5DYcYi-9j6QM-KWQUzt741_k1xon3ZL
"""

# Unsupervised Learning: Clustering
#The task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups
# Examples:
  # Natural Language Processing (NLP)
  # Computer Vision
  # Stock markets
  # Customer/Market Segmentation
#Types:
  # Connectivity-based
    # Distance based
    # E.g. Hierarchical clustering
  # Centroid-based
    # Represents each cluster by a single mean vector
    # e.g. k-means algorithm
  # Distribution-based
    # Modeled using statistical distributions
    #E.g. Multivariate normal distributions used by the expectation-maximization algorithm
  # Density-based
    # Defines clusters as connected dense regions in the data space
    # E.g. DBSCAN

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import itertools
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from mlxtend.classifier import EnsembleVoteClassifier
from mlxtend.data import iris_data
from mlxtend.plotting import plot_decision_regions

#Initializing classifiers
clf1 = LogisticRegression(random_state=0,solver='lbfgs',multi_class='auto')
clf2 = RandomForestClassifier(random_state=0,n_estimators=100)
clf3 = SVC(random_state=0, probability=True, gamma='auto')
eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2,1,1], voting='soft')
labels = ['Logistic Regression', 'Random Forest', 'RBF SVM', 'Ensemble']

#loading some example data
X,Y = iris_data()

X = X[:,[0,2]]

gs = gridspec.GridSpec(2,2)
fig = plt.figure(figsize=(10,8))

for clf, lab, grd in zip([clf1,clf2,clf3,eclf],labels,itertools.product([0,1],repeat=2)):
  clf.fit(X,Y)
  ax = plt.subplot(gs[grd[0],grd[1]])
  fig = plot_decision_regions(X=X, y=Y, clf=clf, legend=2)
  plt.title(lab)
plt.show()

# Hierchical

from sklearn.datasets import make_blobs
X,Y = make_blobs(n_samples=600, centers=5, cluster_std=0.6, random_state=42)

plt.scatter(X[:,0],X[:,1],s=10)

from scipy.cluster.hierarchy import ward, dendrogram, linkage
distance = linkage(X, 'ward')

distance

plt.figure(figsize=(25,10))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
dendrogram(distance,leaf_rotation=90,leaf_font_size=9)
plt.axhline(25,c='k')
plt.show()

plt.title('Hierarchical Clustering Dendrogram (truncated)')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
dendrogram(distance,truncate_mode='lastp',p=5,show_contracted=True,leaf_rotation=0,leaf_font_size=12)

# Given a point, how to know which cluster it belongs?
# By Distance
# Use the fcluster function
from scipy.cluster.hierarchy import fcluster
max_d = 25
clusters = fcluster(distance, max_d, criterion='distance')
clusters

plt.figure(figsize=(10,8))
plt.scatter(X[:,0],X[:,1],c=clusters,cmap='rainbow')
plt.show()

# By Clusters
k = 9
clusters = fcluster(distance, k, criterion='maxclust')

plt.figure(figsize=(10,8))
plt.scatter(X[:,0],X[:,1],c=clusters,cmap='rainbow')
plt.show()

# k-means clustering
#analyse and find patterns/clusters within data
#distance measures

#clusters data by trying to seperate samples in n groups of equal variance
#minimizing a criterion known as the inertia or within-cluster sum-of-squares
#requires the number of clusters to be specified
#scales well
#How does it work:
  # Divides a set of samples into disjoint clusters
  # Each described by the mean of the samples in the cluser
  # The means are commonly called the cluster 'centroids'
  # Note that the centroids are not, in general, points from, although they live in the same space.
  # The K-means algorithm aims to choose centroids that minimize the inertia, or within-cluster sum of squared criterion

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=9)
kmeans.fit(X)

y_kmeans = kmeans.predict(X)

plt.scatter(X[:,0],X[:,1],c=y_kmeans,s=10,cmap='inferno')
centers = kmeans.cluster_centers_
plt.scatter(centers[:,0],centers[:,1],c='green',s=500,alpha=0.7)

from mlxtend.plotting import plot_decision_regions
plot_decision_regions(X,Y,clf=kmeans)

#How to choose the number of clusters?
# The Elbow Method

kmeans.inertia_

sse_ = []
for k in range(1,8):
  kmeans = KMeans(n_clusters=k).fit(X)
  sse_.append([k,kmeans.inertia_])

sse_

import pandas as pd
plt.plot(pd.DataFrame(sse_)[0],pd.DataFrame(sse_)[1])

# Select the number of k where the elbow (knik) occurs

# Silhouette Analysis for secting the value for k
# silhouette score = (p-q)/(max(p,q))
# where p = mean distance to the points in the nearest cluster that the data point is not part of
# and q = mean intra_cluster distance to all point in its own cluster
  # the value of the silhouette score range lies between -1 and 1
  # a score closer to 1 indicates that the data point is very similar to other data points in the cluster
  # a score closter to -1 indicates that the data point is not similar to the data points in its cluster

from sklearn.metrics import silhouette_score

sse_ = []
for k in range(2,8):
  kmeans = KMeans(n_clusters=k).fit(X)
  sse_.append([k,silhouette_score(X,kmeans.labels_)])

plt.plot(pd.DataFrame(sse_)[0],pd.DataFrame(sse_)[1])

# Best K is at the maximum of silhouette score plot

# Mean shift method to select the optimal k values
#Non-paramatic, will identify it for you
# identify centroids location
  # for each data point, it identifies a window around it
  # computes centroid
  # updates centroid location
  # continue to update window
  # keep shifting the centroids, means, towards the peaks of each cluster, Hence the term means shift
  # continues until centroids no longer move
# Used for object tracking

from sklearn.cluster import MeanShift, estimate_bandwidth
from itertools import cycle

bandwith_X = estimate_bandwidth(X,quantile=0.1, n_samples=len(X))
meanshift_model = MeanShift(bandwidth=bandwith_X, bin_seeding=True)
meanshift_model.fit(X)

cluster_centers = meanshift_model.cluster_centers_
print('\nCenters of clusters: \n',cluster_centers)

meanshift_model.labels_

bandwith_X

labels = meanshift_model.labels_
num_clusters = len(np.unique(labels))
print('\nNumber of clusters in input data = \n',num_clusters)

plt.figure(figsize=(10,8))
markers = '*vosx'
for i,marker in zip(range(num_clusters),markers):
  plt.scatter(X[labels==i,0],X[labels==i,1],marker=marker, color='orange')
  cluster_center = cluster_centers[i]
  plt.plot(cluster_center[0],cluster_center[1],'o',markerfacecolor='black',markeredgecolor='black',markersize=15)
plt.title('Clusters')

