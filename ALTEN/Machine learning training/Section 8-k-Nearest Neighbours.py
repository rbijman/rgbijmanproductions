# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zPXSsqkS13pD_qfDPDZjBSUkrzWOus-V
"""

# K-Nearest Neighbor (KNN)
#Supervised classification machine learning
#Lazy learner
  # Instance Based (you have to train it every time you run it)
  # Lazy because it does not try to learn a function from the training data (like the others)
  # it memorise the pattern from the dataset
# Non parametetric
  # Distribution free tests because no assumption of the data needing to follow a specific distribution
# Not efficient on big-data, very susceptible to overfitting

#Steps:
  # choose the number of k
  # select a distance metric
  # find the k nearest neighbours of the sample
  # assign the class label my majority vote

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets

df = sns.load_dataset('iris')
df.head()

X_train = df[['petal_length', 'petal_width']]
species_to_num = {'setosa': 0, 'versicolor': 1, 'virginica': 2}
df['species'] = df['species'].map(species_to_num)
y_train = df['species']

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(X_train, y_train)

Xv = X_train.values.reshape(-1, 1)
h = 0.02
x_min, x_max = Xv.min(), Xv.max() + 1
y_min, y_max = y_train.min(), y_train.max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

xx

ax = plt.contour(xx,yy,Z,cmap='afmhot',alpha=0.3)
plt.scatter(X_train.values[:,0],X_train.values[:,1],c=y_train,s=40,alpha=0.9,edgecolors='k')

from google.colab import files
uploaded = files.upload()

ls

# Project Cancer Detection (KNN)
import numpy as np
import pandas as pd

col = ['id','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data
df = pd.read_csv("breast-cancer-wisconsin.data",names=col,header=None)
df.head()

#Data Pre-processing
np.where(df.isnull())

df.info()

df['Bare Nuclei'].describe()

df['Bare Nuclei'].value_counts()

df[df['Bare Nuclei']=="?"]

df['Class'].value_counts()

df['Bare Nuclei'].replace("?",np.nan,inplace=True)
df = df.dropna()

df.shape

df['Class'] = df.Class/2-1

df.Class.value_counts()

df.columns

df.info()

X = df.drop(['id','Class'],axis=1)
X_col = X.columns
Y = df['Class']

from sklearn.preprocessing import StandardScaler
X = StandardScaler().fit_transform(X.values)
df1 = pd.DataFrame(X,columns=X_col)
df1.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

#Other way of preprocessing
from sklearn.preprocessing import MinMaxScaler
pd.DataFrame(MinMaxScaler().fit_transform(df.drop(['id','Class'],axis=1).values),columns=X_col).head()

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5,p=2,metric='minkowski')
knn.fit(X_train, y_train)

from sklearn import preprocessing
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix, roc_auc_score
def print_score(clf, X_train, y_train, X_test, y_test, train=True):
  lb = preprocessing.LabelBinarizer()
  lb.fit(y_train)
  if train:
    res = clf.predict(X_train)
    print("Train Result:\n================================================")
    print("accuracy score: {0:.4f}\n".format(accuracy_score(y_train, res)))
    print("classification report:\n {}".format(classification_report(y_train, res)))
    print("confusion matrix: \n {}\n".format(confusion_matrix(y_train, res)))
    print("ROC AUC: {:0.4f}\n".format(roc_auc_score(lb.transform(y_train), lb.transform(res))))
    res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')
    print("Average Accuracy: \t {0:.4f}".format(np.mean(res)))
    print("Accuracy SD: \t\t {0:.4f}".format(np.std(res)))
  elif train==False:
    res_test = clf.predict(X_test)
    print("Test Result:\n================================================")
    print("accuracy score: {0:.4f}\n".format(accuracy_score(y_test, res_test)))
    print("classification report:\n {}".format(classification_report(y_test, res_test)))
    print("confusion matrix: \n {}\n".format(confusion_matrix(y_test, res_test)))
    print("ROC AUC: {:0.4f}\n".format(roc_auc_score(lb.transform(y_test), lb.transform(res_test))))

print_score(knn, X_train, y_train, X_test, y_test, train=True)
print_score(knn, X_train, y_train, X_test, y_test, train=False)

#Grid search
from sklearn.model_selection import GridSearchCV
knn.get_params()

params = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10]}

grid_search_cv = GridSearchCV(KNeighborsClassifier(), params, cv=10, n_jobs=-1,verbose=1)
grid_search_cv.fit(X_train, y_train)

grid_search_cv.best_estimator_

print_score(grid_search_cv, X_train, y_train, X_test, y_test, train=True)

print_score(grid_search_cv, X_train, y_train, X_test, y_test, train=False)

grid_search_cv.cv_results_['mean_test_score']

grid_search_cv.cv_results_

#Compare now to SVM and Random Forest
from sklearn import svm
clf = svm.SVC(kernel='rbf', gamma="scale")
clf.fit(X_train, y_train)
print_score(clf, X_train, y_train, X_test, y_test, train=True)
print_score(clf, X_train, y_train, X_test, y_test, train=False)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42,n_estimators=100)
rf.fit(X_train, y_train)
print_score(rf, X_train, y_train, X_test, y_test, train=True)
print_score(rf, X_train, y_train, X_test, y_test, train=False)

